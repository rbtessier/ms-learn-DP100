{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating machine learning with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Azure Machine Learning, you run workloads as experiments that leverage data assets and compute resources. In an enterprise data science process, you'll generally want to separate the overall process into individual tasks, and orchestrate these tasks as *pipelines* of connected steps.\n",
    "\n",
    "## Learning objectives\n",
    "In this module, you'll learn how to:\n",
    "- Create an Azure Machine Learning pipeline.\n",
    "  - Define steps\n",
    "  - define inputs and outputs\n",
    "  - deal with reusability\n",
    "- Publish an Azure Machine Learning pipeline.\n",
    "- Schedule an Azure Machine Learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Introduction\n",
    "A *pipeline* is a workflow of machine learning tasks in which each task is implemented as a step.\n",
    "\n",
    "Steps can be arranged sequentially or in parallel, enabling you to build sophisticated flow logic to orchestrate machine learning operations. Each step can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal. \n",
    "\n",
    "You can publish a pipeline as a REST endpoint, enabling client applications to initiate a pipeline run. You can also define a schedule for a pipeline, and have it run automatically at periodic intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline steps\n",
    "An Azure Machine Learning pipeline consists of one or more *steps* that perform tasks. \n",
    "### Types of steps\n",
    "Common kinds of steps in an Azure Machine Learning pipeline include:\n",
    "- `PythonScriptStep`: Runs a specified Python script\n",
    "- `EstimatorStep`: Runs an estimator\n",
    "- `DataTransferStep`: Uses Azure Data Factory to copy data between data stores.\n",
    "- `DatabricksStep`: Runs a notebook, script, or compiled JAR on a databricks cluster\n",
    "- `AdlaStep`: Runs a U-SQL job in Azure Data Lake Analytics.\n",
    "\n",
    "`Note`: For a full list see [azure.pipeline.steps package documentation](https://aka.ms/AA70rrh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining steps in a pipeline\n",
    "To create a pipeline, you must first define each step and then create a pipeline that includes the steps. See example below to see how the specific configuration of each step can vary by the step type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig=run_config)\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    "                      estimator = sk_estimator,\n",
    "                      compute_target = 'aml-cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After definine the steps, you can assign them to a pipeline and run it as an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Construct the pipeline\n",
    "train_pipeline = Pipeline(workspace = ws, steps = [step1, step2])\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
    "pipeline_run=experiment.submit(train_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass data between pipeline steps\n",
    "Oten a pipeline line includes at least one step that depends on the output of a preceding step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THe PipelineData object\n",
    "The `PipelineData` object is a special kind of `DataReference` that:\n",
    "- References a location in a datastore.\n",
    "- Creates a data dependency between pipeline steps.\n",
    "You can view a `PipelineData` object as an intermediary store for data that must be passed from a step to a subsequent step.\n",
    "\n",
    "<img src =https://docs.microsoft.com/en-us/learn/wwl-data-ai/create-pipelines-in-aml/media/06-01-pipelinedata.jpg>\n",
    "\n",
    "### PipelineData step inputs and outputs\n",
    "To use a `PipelineData` object to pass data between steps, you must:\n",
    "1. Define a named `PipelineData` object that references a location in a datastore.\n",
    "2. Specify the `PipelineData` object as an *input* or *output* for the steps that use it\n",
    "3. Pass the `PipelineData` object as a script parameter in steps that run scripts (and include code in those scripts to read or write data)\n",
    "\n",
    "For example, the following code defines a `PipelineData` object that for the preprocessed data that must be passed between the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Get a dataset for the initial data\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "# Define a PipelineData object to pass data between steps\n",
    "data_store = ws.get_default_datastore()\n",
    "prepped_data=PipelineData('prepped', datastore=data_store)\n",
    "\n",
    "#step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory= 'scripts',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         # Specify Dataset as initial input\n",
    "                         inputs = [raw_ds.as_named_input('raw_data')],\n",
    "                         # Specify PipelineData as output\n",
    "                         outputs = [prepped_data],\n",
    "                         # Also pass as data reference to script\n",
    "                         arguments = ['--folder', prepped_data])\n",
    "\n",
    "# Step to run as estimator\n",
    "step2 = EstimatorStep(name = 'train model', \n",
    "                      estimator = sk_estimator,\n",
    "                      compute_target = 'aml-cluster',\n",
    "                      #Specify PipelineData as input\n",
    "                      inputs = [prepped_data],\n",
    "                      # Pass as data reference to estimator script\n",
    "                      estimator_entry_script_arguments=['--folder', prepped_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scripts themselves, you can obtain a reference to the `PipelineData` object from the scrip argument and use it like a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "#Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get input dataset as dataframe\n",
    "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.addargument('--folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "#code to prep data (in this case, just select specific columns)\n",
    "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
    "prepped_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse pipeline steps\n",
    "Pipelines with multiple long-running steps can take a significant time to complete. Azure Machine Learning includes some caching and reuse features to reduce this time.\n",
    "\n",
    "### Managing step output reuse\n",
    "By default, the step output from a previous pipeline run is reused without rerunning th estep provided the script, source directory, and othe rparameters for th estep have not changed. This can lead to stale results when changes to downstream data sources have not been accounted for.\n",
    "\n",
    "To controle for this you can set the `allow_reuse` parameter in the step configuration like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
    "                         outputs=[prepped_data],\n",
    "                         arguments = ['--folder', prepped_data]),\n",
    "                         # Disable step reuse\n",
    "                         allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have multiple steps, you can force all of them to run regardless of individual reuse configuration by setting the `regenerate_outputs` parameter when submitting the pipeline experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulbish Pipelines\n",
    "After you have created a pipeline, you can publish it to create a REST endpoint through which the pipeline can be run on demand.\n",
    "### Publishing a pipeline\n",
    "To publish a pipeline, you can call its `publish` method, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can call the `publish` method on a successfull run of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "# Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pipeline has been published, you can view it in Azure Machine Learning Studio. You can also determine the URI of its endpoint like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Published pipeline\n",
    "To initiate a published enpoint, you make an HTTP request to its REST endpoint, passing an authorization header with a token for a service principal with permission to run the pipelin, and a JSON payload specifying the xperiment name. The pipeline is run asynchronously, so th eresponse from a successful REST call includes the run ID. You can use this to track the run in Azure Machine Learning studio.\n",
    "\n",
    "For example, the following Python code makes a REST request to run a pipeline and displays the returned run ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pipeline parameters\n",
    "you can increase the flexibility of a pipeline by defining parameters.\n",
    "\n",
    "### Defining parameters for a pipeline\n",
    "\n",
    "To define parameters for a pipeline, create a `PipelineParameter` object for each parameter, and specify each parameter in at least one step.\n",
    "\n",
    "For example, regularization rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)\n",
    "\n",
    "...\n",
    "\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    "                      estimator = sk_estimator,\n",
    "                      compute_target = 'aml-cluster',\n",
    "                      inputs=[prepped],\n",
    "                      estimator_entry_script_arguments=['--folder', prepped,\n",
    "                                                        '--reg', reg_param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SChedule pipelines\n",
    "After publishing, you can initiate the pipeline on demand through its REST endpoint, or you can have the pipeline run automatically based on a periodic schedule or in response to data updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling a pipeline for periodic intervals\n",
    "To schedule to run at periodic intervals, you must define a `ScheduleRecurrence` that determines the run frequency, and use it to create a `Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
    "                                        description='trains model every day',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Training_Pipeline',\n",
    "                                        recurrence=daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggering a pipeline run on data changes\n",
    "\n",
    "To schedule a pipeline to run whenever data changes, you must create a `Schedule` that monitors a specified path on a datastore like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline_id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    datastore=training_datastore,\n",
    "                                    path_on_datastore='data/training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
