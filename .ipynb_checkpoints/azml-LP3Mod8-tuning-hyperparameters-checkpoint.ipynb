{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters\n",
    "\n",
    "## Learning objectives\n",
    "In this module, you will learn how to:\n",
    "\n",
    "- Define a hyperparameter search space.\n",
    "- Configure hyperparameter sampling.\n",
    "- Select an early-termination policy.\n",
    "- Run a hyperparameter tuning experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Search space\n",
    "\n",
    "Set of hyperparameter values tried during hyperparameter tuning is known as the *search space*.\n",
    "\n",
    "### Discrete hyperpameters\n",
    "\n",
    "You can define a search space from a choice of explicit values using `choice` - ie `list(choice([10, 20, 30]))` a `range`:  `(choice(range(1,10)))`, or an arbitrary set of comma-separated values `(choice(30,50,100))`\n",
    "\n",
    "You can also select discrete values from any of the following discrete distributions:\n",
    "\n",
    "- qnormal\n",
    "- quniform\n",
    "- qlognormal\n",
    "- qloguniform\n",
    "\n",
    "### Continuous hyperparameters\n",
    "\n",
    "Some hyperparameters are continuous - in other words you can use any value along a scale. To define a search space for these kinds of value, you can use any of the following distribution types:\n",
    "\n",
    "- normal\n",
    "- uniform\n",
    "- lognormal\n",
    "- loguniform\n",
    "\n",
    "Example of defining a search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import choice, normal\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Sampling\n",
    "\n",
    "## 1. Grid Sampling\n",
    "\n",
    "Grid sampling can only be employed when all hyperparameters are discrete, and is used to try every possible combination of parameters in the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, choice\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': choice(0.01, 0.1, 1.0)\n",
    "              }\n",
    "\n",
    "param_sampling = GridParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Sampling\n",
    "Random sampling is used to randomly select a value for each hyperparameter, which can be a mix of discrete and continuous values as shown in the following code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, normal\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }\n",
    "\n",
    "param_sampling = RandomParabmeterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian sampling\n",
    "Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection. The following code example shows how to configure Bayesian sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': uniform(0.5, 0.1)\n",
    "              }\n",
    "\n",
    "param_sampling = BayesianParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can only use Bayesian sampling with choice, uniform, and quniform parameter expressions, and you can't combine it with an early-termination policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring early termination\n",
    "\n",
    "With a sufficiently large hyperparameter search space, it could take many iterations (child runs) to try every possible combination. Typically, you set a maximum number of iterations, but this could still result in a large number of runs that don't result in a better model than a combination that has already been tried.\n",
    "\n",
    "To help prevent wasting time, you can set an early termination policy that abandons runs that are unlikely to produce a better result than previously completed runs. The policy is evaluated at an evaluation_interval you specify, based on each time the target performance metric is logged. You can also set a delay_evaluation parameter to avoid evaluating the policy until a minimum number of iterations have been completed.\n",
    "\n",
    "`Note: Early termination is particularly useful for deep learning scenarios where a deep neural network (DNN) is trained iteratively over a number of epochs. The training script can report the target metric after each epoch, and if the run is significantly underperforming previous runs after the same number of intervals, it can be abandoned.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bandit policy\n",
    "You can use a bandit policy to stop a run if the target performance metric underperforms the best run so far by a specified margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_amount = 0.2,\n",
    "                                        evaluation_interval=1,\n",
    "                                        delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example applies the policy for every iteration after the first five, and abandons runs where the reported target metric is 0.2 or more worse than the best performing run after the same number of intervals.\n",
    "\n",
    "You can also apply a bandit policy using a slack *factor*, which compares the performance metric as a ratio rather than an absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Median stopping policy\n",
    "A median stopping policy abandons runs where the target performance metric is worse than the median of the running averages for all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,\n",
    "                                                delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Truncation selection policy\n",
    "A truncation selection policy cancels the lowest performing *X%* of runs at each evaluation interval based on the `truncation_percentage` value you specify for *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "\n",
    "early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,\n",
    "                                                     evaluation_interval=1,\n",
    "                                                     delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training script for hyperparameter tuning\n",
    "To run a hyperdrive experiment, you need to create a training script just the way you would do for any other training experiment, except that your script must:\n",
    "\n",
    "- Include an argument for each hyperparameter you want to vary.\n",
    "- Log the target performance metric. This enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.\n",
    "\n",
    "For example, the following example script trains a logistic regression model using a --regularization argument to set the regularization rate hyperparameter, and logs the accuracy metric with the name Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all pretty standard\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the training dataset\n",
    "data = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels, and split for training/validatiom\n",
    "X = data[['feature1','feature2','feature3','feature4']].values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model with the reg hyperparameter\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate and log accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring and running a hyperdrive experiment\n",
    "To prepare the hyperdrive experiment, you must use a **HyperDriveConfig** object to configure the experiment run, as shown in the following example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "\n",
    "# Assumes ws, sklearn_estimator and param_sampling are already defined\n",
    "\n",
    "hyperdrive = HyperDriveConfig(estimator=sklearn_estimator,\n",
    "                              hyperparameter_sampling=param_sampling,\n",
    "                              policy=None,\n",
    "                              primary_metric_name='Accuracy',\n",
    "                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                              max_total_runs=6,\n",
    "                              max_concurrent_runs=4)\n",
    "\n",
    "experiment = Experiment(workspace = ws, name = 'hyperdrive_training')\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and reviewing hyperdrive runs\n",
    "You can monitor hyperdrive experiments in Azure Machine Learning studio, or by using the Jupyter Notebooks RunDetails widget.\n",
    "\n",
    "The experiment will initiate a child run for each hyperparameter combination to be tried, and you can retrieve the logged metrics these runs using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child_run in run.get_children():\n",
    "    print(child_run.id, child_run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also list all runs in descending order of performance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child_run in hyperdrive_.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the best performing run, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
