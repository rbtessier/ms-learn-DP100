{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data in Azure Machine Learning\n",
    "This is a notebook for notes from the online modules from the DP-100 training in Azure. I think I did mod 2 in a myCIWorkstationRyan Jupyter Notebook which is why there's nothing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes I would wonder if I had managed to use the Python SDK but this proves it works on my computer, for a while I forget I had done the First Mod on this.\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myCIWorkstationRyan : ComputeInstance\n",
      "myCClusterRyan : AmlCompute\n"
     ]
    }
   ],
   "source": [
    "for compute_name in ws.compute_targets:\n",
    "    compute = ws.compute_targets[compute_name]\n",
    "    print(compute.name, \":\", compute.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blob_data\n",
      "azureml_globaldatasets\n",
      "workspacefilestore\n",
      "workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "for ds_name in ws.datastores:\n",
    "    print(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register new blob datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(workspace = ws,\n",
    "                                                 datastore_name='blob_data',\n",
    "                                                 container_name='data_container',\n",
    "                                                 account_name='az_store_acct',\n",
    "                                                 account_key='12345678')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blob_data\n",
      "azureml_globaldatasets\n",
      "workspacefilestore\n",
      "workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "for ds_name in ws.datastores:\n",
    "    print(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the workspaces\n",
    "workspace_blob_store = Datastore.get(ws, datastore_name='workspaceblobstore')\n",
    "blob_store = Datastore.get(ws, datastore_name = 'blob_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"blob_data\",\n",
      "  \"container_name\": \"data_container\",\n",
      "  \"account_name\": \"az_store_acct\",\n",
      "  \"protocol\": \"https\",\n",
      "  \"endpoint\": \"core.windows.net\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#gets the default datastore\n",
    "default_store = ws.get_default_datastore()\n",
    "print(default_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cca8fbd8b38c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sets a new default datastore - go to portal to view this change - does this change where the notebook goes to?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_default_datastore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'workspaceblobstore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ws' is not defined"
     ]
    }
   ],
   "source": [
    "#sets a new default datastore - go to portal to view this change - does this change where the notebook goes to?\n",
    "ws.set_default_datastore('workspaceblobstore')\n",
    "type(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working directly with a datastore\n",
    "You can work directly with a datastore to upload and download data by using the methods of the **datastore** class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blob_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4b1a044c53a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m blob_store.upload(src_dir = './digits-recognizer-kaggle/data',\n\u001b[0m\u001b[0;32m      2\u001b[0m                  \u001b[0mtarget_path\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'digits-recognizer-kaggle/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  overwrite = True, show_progress=True)\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#uploads a whole directory src_dr, could also use blob_store.upload_files(files = ['./digits-recognizer-kaggle/train.csv', 'digits-recognizer-kaggle/test.csv'], ...)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#this didn't work for me so I just decided to copy the learning information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'blob_store' is not defined"
     ]
    }
   ],
   "source": [
    "blob_store.upload(src_dir = './digits-recognizer-kaggle/data',\n",
    "                 target_path= 'digits-recognizer-kaggle/',\n",
    "                 overwrite = True, show_progress=True)\n",
    "#uploads a whole directory src_dr, could also use blob_store.upload_files(files = ['./digits-recognizer-kaggle/train.csv', 'digits-recognizer-kaggle/test.csv'], ...)\n",
    "#this didn't work for me so I just decided to copy the learning information\n",
    "\n",
    "blob_store.download(target_path='downloads',\n",
    "                   prefix = '/data',\n",
    "                    show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data References\n",
    "When you want to use a datasore in an experiment script, you must pass a data reference to the script. The data reference is configured for one of the following data access modes:\n",
    "- **Download:** The contents of the path associated with teh data reference is downloaded toteh compute context where the experiment is running\n",
    "- **Upload:** The files generated by your experiment script are uploaded to teh datastore after the run completes\n",
    "- **Mount:** the path on the datastore is mounted as remote storage in the experiment compute context, enabling the contents to be accessed remotely (note that this mode is only available when teh experiment is run on a remote compute target - you cannot use this mode with local compute).\n",
    "\n",
    "To pass the data reference to an experiment script, you must define a script parameter as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-53b53ffc8143>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-53b53ffc8143>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    compute_target='local',\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data_ref = blob_ds.path('data/files').as_download(path_on_compute='training_data')\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "                    entry_script='training_script.py'\n",
    "                    compute_target='local',\n",
    "                    script_params = {'--data_folder': data_ref})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your training script, you can retrieve the parameter and use it like a local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_folder DATA_FOLDER]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Ryan\\AppData\\Roaming\\jupyter\\runtime\\kernel-8ed8c5db-0768-46bf-b147-49fe6370b133.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', type=str, dest='data_folder')\n",
    "args = parser.parse_args()\n",
    "data_files = os.listdir(args.data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Datasets\n",
    "*Datasets* are versioned packaged data objects that can be easily consumed in experiments and pipelines. Datasets are the recommended way to work with data, and are the primary mechanism for advanced Azure Machine Learning capabilities like data labeling and data drift monitoring.\n",
    "## Types of Datasets\n",
    "Datasets are typically based on files in a datastore, though thaey can also be based on URLs and other sources. You can create the following types of dataset:\n",
    "- **Tabular:** The data is read from the dataset as a table. You should use this type of dataset when your data is consistently structured and you want to work with it in common tabular data structures, such as Pandas dataframes.\n",
    "- **Files:** The dataset presents a list of file paths that can be read as though from the file system. Use this type of dataset when your data is unstructured, or when you need to process the data at the file level (for example, to train a convolutional neural network from a set of image files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and registering datasets\n",
    "You can use the visual interface in Azure Machine Learning studio or the Azure Machine Learning SDK to create datasets from individual files or multiple file paths. The paths can include wildcards (for example, /files/*.csv) making it possible to encapsulate data from a large number of files in a single dataset.\n",
    "\n",
    "After you've created a dataset, you can register it in the workspace to make it available for use in experiments and data processing pipelines later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Registering tabular datasets\n",
    "To create a tabular dataset using the SDK, use ```from_delimited_files``` method of the ```Dataset.Tabular``` class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "csv_paths = [(blob_ds, 'data/files/current_data.csv'),\n",
    "             (blob_ds, 'data/files/archive/*.csv')]\n",
    "tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)\n",
    "tab_ds = tab_ds.register(workspace=ws, name='csv_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset in this example includes data from two file paths within the default datastore:\n",
    "- the ```current_data.csv``` file in the ```data/files``` folder.\n",
    "- all .csv files in the ```data/files/archive/``` folder.\n",
    "\n",
    "After creating the dataset, the code registers it in the workspace with the name ```csv_table```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Registering file datasets\n",
    "To create a file dataset using the SDK< use the ```from_files``` method of the ```Dataset.File``` class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset in the example includes all .jpg files in the ```data/files/images``` path within the default datastore:\n",
    "\n",
    "After creating the dataset, the code registers it in the workspace with the name ```img_files```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a registered dataset\n",
    "After registering a dataset, you can retrieve it by using any of the following techniques:\n",
    "- the ```datasets``` dictionary attribute of a ```Workspace``` object.\n",
    "- the ```get_by_name``` or ```get_by_id``` method of the ```Dataset``` class.\n",
    "\n",
    "Both of these techniques are shown in the following code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a dataset from the workspace datasets collection\n",
    "ds1 = ws.datasets['csv_table']\n",
    "\n",
    "# Get a dataset by name from the datasets class\n",
    "ds2 = Dataset.get_by_name(ws, 'img_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset versioning\n",
    "Datasets can be versioned, enabling you to track historical versions of datasets that were used in experiments, and reproduce those experiments with data in the same state.\n",
    "\n",
    "You can create a new version of a dataset by registering it with the same name as a previously registered dataset and specifying the ```create_new_version``` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [(blob_ds, 'data/files/images/*.jpg'),\n",
    "             (blob_ds, 'data/files/images/*.png')]\n",
    "file_ds = Dataset.File.from_files(path=img_paths)\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the .png files in the ```images``` folder have been added to the definition of the ```img_paths``` dataset example used in the previous topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a specific dataset version\n",
    "You can retrieve a specific version of a dataset by specifying the ```version``` parameter in the ```get_by_name``` method of the ```Dataset``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use datasets\n",
    "You can read data directly from a dataset, or you can pass a dataset as a named *input* to a script configuration or estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a dataset directly\n",
    "If you have a reference to a dataset, you can access its contents directly.\n",
    "\n",
    "For tabular datasets, most data processing begins by reading the dataset as a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tab_ds.to_pandas_dataframe()\n",
    "# code to work with dataframe goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a file dataset, you can use the ```to_path()``` method to return a list of the file paths encapsulated by the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_ds.to_path():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing a dataset to an experiment script\n",
    "When you need to access a dataset in an experiment script, you can pass the dataset as an input to a ```ScriptRunConfig``` or an ```Estimator```. For example, the following code passes a tabular dataset to an estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SKLearn( source_directory='experiment_folder',\n",
    "                     entry_script='training_script.py',\n",
    "                     compute_target='local',\n",
    "                     inputs=[tab_ds.as_named_input('csv_data')],\n",
    "                     pip_packages=['azureml-dataprep[pandas]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the script will need to work with a ```Dataset``` object, you must include either the full ```azureml-sdk``` package or the ```azureml-dataprep``` package with the ```pandas``` extra library in the script's compute environment.\n",
    "\n",
    "In the experiment script itself, you can access the input and work with the ```Dataset``` object it references like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Run.get_context()\n",
    "data = run.input_datasets['csv_data'].to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passing a file dataset, you must specify the access mode. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator( source_directory='experiment_folder',\n",
    "                     entry_script='training_script.py'\n",
    "                     compute_target='local',\n",
    "                     inputs=[img_ds.as_named_input('img_data').as_download(path_on_compute='data')],\n",
    "                     pip_packages=['azureml-dataprep[pandas]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
