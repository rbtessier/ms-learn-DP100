{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As machine learning becomes increasingly integral to decisions that affect health, safety, economic wellbeing, and other aspects of people's lives, it's important to be able to understand how models make predictions; and to be able to explain the rationale for machine learning based decisions while identifying and mitigating bias.\n",
    "\n",
    "Explaining models is difficult because of the range of machine learning algorithm types and the nature of how machine learning works, but model interpretability has become a key element of helping to make model predictions explainable.\n",
    "\n",
    "## Learning objectives\n",
    "In this module, you will learn how to:\n",
    "\n",
    "- Interpret *global* and *local* feature importance.\n",
    "- Use an explainer to interpret a model.\n",
    "- Create model explanations in a training experiment.\n",
    "- Visualize model explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Model explainers use statistical techniques to calculate feature importance. This enables you to quantify the relative influence each feature in the training dataset has on label prediction. **Explainers work by evaluating a test data set of feature cases and the labels the model predicts for them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global feature importance\n",
    "Global feature importance quantifies the relative importance of each feature in the test dataset as a whole. It provides a general comparison of the extent to which each feature in the dataset influences prediction.\n",
    "\n",
    "For example, a binary classification model to predict loan default risk might be trained from features such as loan amount, income, marital status, and age to predict a label of 1 for loans that are likely to be repaid, and 0 for loans that have a significant risk of default (and therefore shouldn't be approved). An explainer might then use a sufficiently representative test dataset to produce the following global feature importance values:\n",
    "\n",
    "- **income**: 0.98\n",
    "- **loan amount**: 0.67\n",
    "- **age**: 0.54\n",
    "- **marital status** 0.32\n",
    "Global feature importance chart\n",
    "\n",
    "<img src=https://docs.microsoft.com/en-us/learn/wwl-data-ai/explain-machine-learning-models-with-azure-machine-learning/media/09-01-global-importance.png>\n",
    "\n",
    "It's clear from these values, that in respect to the overall predictions generated by the model for the test dataset, **income** is the most important feature for predicting whether or not a borrower will default on a loan, followed by the **loan** amount, then **age**, and finally **marital status**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local feature importance\n",
    "**Local feature importance** measures the influence of each feature value for a specific individual prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
