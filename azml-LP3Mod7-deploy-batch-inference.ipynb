{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy batch inference pipelines with Azure Machine Learning\n",
    "In many production scenarios, long-running tasks that operate on large volumes of data are performed as *batch* operations. In machine learning, *batch inferencing* is used to apply a predictive model to multiple cases asynchronously - usually writing the results to a file or database. Typically this is done by creating a pipeline.\n",
    "\n",
    "## Learning objectives\n",
    "- Public Batch inference pipeline for a trained model.\n",
    "- Use a batch inference pipeline to generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a batch inference pipeline\n",
    "1. Register a model - same as in real-time inferencing\n",
    "2. Create a scoring script (aka entry script) - difference here is that un the `run` function, we pass `mini_batch` for mini batches and process in a list. Run is called for each batch of data to be processed\n",
    "3. Create a pipeline with a ParallelRunStep - we dont' need to define a pipeline in real-time inferencing\n",
    "4. Run the pipeline and retrieve the step output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a pipeline with a ParallelRunStep\n",
    "Azure Machine Learning provides a type of pipeline step specifically for performing parallel batch inferencing. Using the `ParallelRunStep` class, you cna read batches of files froma `File` dataset and write the processing output to a `PipelineData` reference. Additoinally, you can set the `output_action` setting for the step to \"append_row\", which will ensure that all instances of the step being run in parallel will collate their results to a single output file names *parallel_run_step.txt*. The following code snipppet shows an example of creating a pipeline with a `ParallelRunStep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azurem.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the batch dataset for input\n",
    "batch_data_set = ws.datasets('batch-data')\n",
    "\n",
    "# Set the output location\n",
    "default_ds = ws.get_default_datastore()\n",
    "output_dir = PipelineData(name='inferences',\n",
    "                          datastore=default_ds,\n",
    "                          output_path_on_compute='results')\n",
    "\n",
    "# Define the parallel run step step configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='batch_scripts',\n",
    "    entry_script=\"batch_scoring_script.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=aml_cluster,\n",
    "    node_count=4)\n",
    "\n",
    "# Create the parallel run step\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('batch_data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the pipeline and retrieve the step output\n",
    "After your pipeline ahs been defined, you can run it and wait for it to complete. Then you can retrieve the `parallel_run_step.txt` files from the output of the step to view the results as shown in the following code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-173409e2ea90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Run the pipeline as an experiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpipeline_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch_prediction_pipeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpipeline_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ws' is not defined"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Run the pipeline as an experiment\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get the outputs from the first (and only) step\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='results')\n",
    "\n",
    "# Find the parallel_run_step.txt file\n",
    "for root, dirs, files in os.walk('results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing a batch inference pipeline\n",
    "You can publish a batch inferencing pipeline as a REST service, as shown in the following example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
    "                                                   description='Batch pipeline',\n",
    "                                                   version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once published, you can use the service endpoint to initiate a batch inferencing job, as shown in the following example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"Batch_Prediction\"})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also schedule the published pipeline to have it run automaticlay, as shown in the following example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
    "                                        description='batch inferencing',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Batch_Prediction',\n",
    "                                        recurrence=weekly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
